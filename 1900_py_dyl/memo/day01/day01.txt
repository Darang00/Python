웹 크롤링
- 웹 상에 존재하는 데이터를 수집하는 것	
- 웹 페이지에 보이는 데이터 중 원하는 데이터를 수집, 추출하는 과정

html: mark up langauge (프로그래밍 언어는 아니고, 화면에 뿌리는 언어)



네트워크 (통신망)
	두 대 이상의 컴퓨터 같은 기기들이 연결되어 통신하는 체계

인터넷
	여러 네트워크들을 연결시켜 전세계를 연결한 거대한 네트워크

웹 (WWW: World Wide Web)
	인터넷 상에서 동작하는 서비스
	서로 데이터를 주고받을 수 있는 거대한 공간
	서로 정보를 공유할 수 있는 공간.	
	ex) web browser: edge, chrome
	웹 브라우저를 통해 사용자에게 여러 데이터를 제공한다.


웹 페이지
	- 웹 상에 존재하며 특정 정보를 담고있는 문서
	- 일반 책의 페이지와는 다르게 HyperLink 라는 것을 사용하여
	다른 페이지로 빠르게 넘어갈 수 있다.
	HyperLink는 HyperText 에서 사용이 가능하다.
	- HTML(Hyper Text MarkUp Language), CSS, JavaScript 등으로 만들어진다.

웹의 구성요소
	서버
	  서비스를 제공하는 기기

	클라이언트
	  서비스를 제공받는 기기


웹 사이트를 접속한다는 것은 웹 브라우저를 통해 서버에 요청(Request)을 보내는 것이며,
서버에서는 그에 따른 응답(Response)을 클라이언트에게 보내주는 것이다.



==================================================================
웹 크롤링 방법
1.  웹 페이지 정보를 가져온다. (requests  모듈 사용)
2. HTML 코드를 파싱(분석)하여 원하는 정보를 뽑아낸다(BeautifulSoup 모듈 사용). 

네이버 url을 활용하여 requests를 보내면 응답으로 해당 html코드가 오게되고, 
우리는 그 코드를 BeautifulSoup으로 분석하여 원하는 데이터만 뽑아내면 된다.

크롤링 주의사항
- 법적으로 문제가 될 수 있음.. (남의 사이트에서 뽑은 데이터로 돈벌면 안됨)
1. 상업적인 용도로 사용하거나 불법적인 부분에서 사용할 경우 법적 문제 발생 가능이 
있으므로 주의한다.
2. 무분별한 웹 크롤링은 웹서버에 과부하 문제를 유발할 수 있으므로 일부 사이트는 크롤링을
막아두거나 일부 페이지만 허용하는 경우가 많다.
그러므로 크롤링하기 전에 항상 /robots.txt 파일을 확인하는게 좋다.
ex) https://www.naver.com/robots.txt 로 url 검색하면 크롤링 못하게 막아놓는 데이터 확인 가능

https://서버주소/robots.txt

robots.txt 의 몇가지 속성
	1. 웹 사이트에서 크롤링으로 정보를 수집할 때 허용 여부를 알려주는 텍스트 파일이다.
	2. 주요 내용은 다음과 같다
	   User-agent: 어떤 크롤러를 허용할 것인지 작성
	   Allow: 크롤링을 허용하는 특정 페이지
	   Disallow:  크롤링을 허용하지 않는 특정 페이지


https://www.useragentstring.com/
	   























































	



























	